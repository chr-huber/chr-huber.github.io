---
title: "Reproducibility in Management Science"
author: "(with M. Fišar, B. Greiner, E. Katok, and the Management Science Reproducibility Collaboration)"
date: "14 December 2023"
date-format: "DD MMMM YYYY"
subtitle: "Christoph Huber"
format: 
  revealjs:
      theme: [white, ch.scss]
      width: 1200
      slide-number: true
      transition: none
      footer: Christoph Huber (WU Vienna)
      template-partials:
        - title-slide.html
              
---


## Outline {.center}

::: {.incremental}

- About me

- Background

- Reproducibility in Management Science

- Future directions

:::


## About me {auto-animate=true background-color="#003366" transition="fade-in" transition-speed="slow"}

Christoph Huber

- PhD in 2021 at University of Innsbruck
- Since Sep. 2021: Postdoc at WU Vienna

Research:

- experimental methods $\longleftrightarrow$ financial economics
- generalizability, replicability, and credibility in (social science) research


## Background {auto-animate=true background-color="#003366"}

- experimental methods $\longleftrightarrow$ financial economics
- generalizability, replicability, and credibility in (social science) research


## Background {background-color="#335c85" visibility="uncounted"}

- experimental methods $\longleftrightarrow$ financial economics
- <u>generalizability</u>, replicability, and credibility in (social science) research

__Generalizability:__

::: {.incremental}
  - e.g.: Do experimental results with student participants hold in the field / with _real_ people?
  
  - Weitzel, Huber, Huber, et al. (RFS): Bubbles and Financial Professionals
    - lab-in-the-field experiment with financial professionals
    - treatment effects from student are generalizable for financial professionals:
      - high liquidity (short-selling) leads less (more) efficient prices
    - but effect sizes are significantly smaller for professionals
:::

## Background {background-color="#1a4775"}

- experimental methods $\longleftrightarrow$ financial economics
- generalizability, <u>replicability</u>, and credibility in (social science) research

__Replicability:__

::: {.incremental}
  - e.g.: Does a(n experimental) phenomenon hold in different data set?
  
  - Füllbrunn, Huber, Eckel, et al. (JFQA): Heterogeneity of Beliefs and Trading Behavior: A Reexamination
    - reexamination of previous results originating from 6 experimental markets from 1 study 
    - same analysis, but much larger data set: 255 exp. markets from 7 studies
    - majority of results holds, but some do not
:::

## Background {background-color="#003366"}

- experimental methods $\longleftrightarrow$ financial economics
- generalizability, replicability, and <u>credibility</u> in (social science) research

__Credibility:__

::: {.incremental}
  - e.g.: Can published results be recreated (_reproduced_) from the original code and data? 
  
    - minimum criterion for the credibility of scientific results<br><br>
  
  - This paper: <br>
:::

. . . 

::: {style="text-align:center;"}
$\rightarrow$ **Reproducibility in Management Science**
:::

::: {.notes}

    - reexamination of previous results originating from 6 experimental markets from 1 study 
    - same analysis, but much larger data set: 255 exp. markets from 7 studies
    - majority of results holds, but some do not

- address external validity concerns 
  - e.g., by conducting artefactual and framed field experiments in addition to standard lab experiments (e.g., experiments with financial professionals)

- study established questions in Economics and Finance with innovative methods and research designs
  - e.g., crowd-science approaches and meta analyses

- take on novel questions originating from recent developments in the field 
  - e.g, cryptocurrency markets; or how the COVID-19 crash affected decision making, topics in the context of sustainable finance

:::


## Introduction

(Social science) Research results need to be robust, such that policies/actions can be based upon them (Ioannidis, 2005; Ioannidis et al., 2013).

:::: {.columns}

::: {.column .incremental width="50%"}

__Replication Crisis__

::: {.small}
- Open Science Collaboration (2015)
  - 35 out of 97 psychology papers with significant effects, could be replicated.

- Camerer et al. (2016)
  - 11 out of 18 economics lab experiments, could be replicated (with 66\% of the original effect size, on average).
  
- Camerer et al. (2018)
  - 13 out of 21 social science experiments in Nature/Science replicated

:::

::: 

::: {.column width="50%"}
:::

::::

## Introduction {visibility="uncounted"}

(Social science) Research results need to be robust, such that policies/actions can be based upon them (Ioannidis, 2005; Ioannidis et al., 2013).

:::: {.columns}

::: {.column width="50%"}

__Replication Crisis__

::: {.small}
- Open Science Collaboration (2015)
  - 35 out of 97 psychology papers with significant effects, could be replicated.

- Camerer et al. (2016)
  - 11 out of 18 economics lab experiments, could be replicated (with 66\% of the original effect size, on average).
  
- Camerer et al. (2018)
  - 13 out of 21 social science experiments in Nature/Science replicated

:::

::: 

::: {.column .incremental width="50%"}

__Reasons for Replicability Issues__

- publication bias (De Long & Lang, 1992)
- HARKing (Hypothesizing After Results are Known) (Kerr, 1998)
- p-hacking ((Brodeur et al., 2016)
- under-powered studies (Szucs & Ioannidis, 2017; Benjamin et al., 2018)
- fraud and dishonesty (John et al., 2012)

:::

::::




## Introduction

__Replication__

- $f_R(data, code'(method')) \approx \beta$
- $f_R(data', code'(method)) \approx \beta$
- $f_R(data', code'(method')) \approx \beta$

::: aside
(Perignon et al., 2023)
:::

## Introduction {visisbility="uncounted"}

__Replication__

- <a style="color: grey;">$f_R(data, code'(method')) \approx \beta$</a>
- <a style="color: grey;">$f_R(data', code'(method)) \approx \beta$</a>
- <a style="color: grey;">$f_R(data', code'(method')) \approx \beta$</a>


This paper $\rightarrow$ __Reproduction__

::: {.incremental}
- $f_R(data, code) = \beta$
- $f_R(data, code) \approx \beta$
- <a style="color: grey;">($f_R(data, code′(method)) \approx \beta$)</a>
:::

::: aside
(Perignon et al., 2023)
:::

## Reproducibility


__Reproducibility__

- an antecedent of replicability/generalizability

- minimum criterion for scientific credibility^[Christensen & Miguel, 2018]

- should not be affected by "questionable research practices" (publication bias, p-hacking, HARKing, under-powered sample sizes)

<br><br>
If reproducibility is not possible (e.g. missing data, missing code, missing information, false reporting), then the replication crisis is partly also a reproducibility crisis.

::: {.notes}

This table displays several types of reanalyses used in empirical economics and finance. Each case pertains to a test of whether the original result β in a given article can be regenerated through the reanalysis process fR, mapping the data and the code, which is a function of the method, to the result βR. Reanalyses differ in that they use the same or different data (d vs. d′), the same or different codes (c vs. c′), and the same or different methods (m vs. m′). When the same code is used, we omit the method argument as it is, by definition, identical to the one used in the original study. Depending on the definition, the test is whether the original result (β) and the regenerated result (βR) are either equal (=) or similar (≈). Sources: National Academies of Sciences, Engineering, and Medicine, 2019; Harvey, 2019; Welch, 2019.

:::



## Disclosure Policies

- Code/data disclosure policies in economics

  - AEA since 2004, JPE & Econometrica followed; QJE since 2016
  
- Mandatory internal (e.g., AEA) or external (e.g., AJPS) verification.

::: {.incremental}
- Finance:
  - Journal of Financial Economics: requires code + data, has data editor
  - Journal of Finance, Review of Financial Studies, Review of Finance: require only code
  - JFQA, Journal of Banking and Finance: no code/data policy
:::

. . .

- Articles in journals with reproducibility policies more often cited (Höffler, 2017, AER)

. . .

- Journal data sharing policies in economics have reduced statistical significance/t-values in papers, and (suggestive evidence) decreased publication bias. (Askarov et al., 2023, JEEA)


## Previous studies on reproducibility

- Dewald et al. (1986, J Money Credit Banking): could only examine 5 out of 154 articles.

- McCullough et al. (2006): despite policy at JMCB, for only 62 out of 193 papers reproduction could be attempted, and 14 (22.6%) could be reproduced.

- Chang and Li (2016): 67 empirical papers in macroeconomics: 33% reproduced independently of authors, 49% when excluding papers with confidential data/no software, and getting help from authors, 68% when only considering journals with data policy.

- Herbert et al. (2021): 303 studies AEJ Applied Economics 2009-2018. only 162 contained non-confidential/non-proprietary data. 68 of these could be reproduced (42.0%), and another 69 (42.6%) were ”partially successful”. 

- Pérignon et al. (2023, empirical finance): Assess replication materials of 168 analyst teams independently testing the same six hypotheses on the same data (Menkveld et al., 2023). Out of 1008 tests, 524 (52.0%) were fully reproducible, and 114 (11.3%) had results with small differences.


## Management Science

- Top5 in management (Q1 in economic and operation research)

  - 14 different departments: Finance, Behavioral Economics and Decision Sciences, Accounting, Operations Management, Marketing, Entrepreneurship/Innovation, Organization, etc.
  
- Receives >4000 submissions per year, publishes about 350 articles per year


## Management Science Code & Data Disclosure Policy

- Until June 2019 ("_Before 2019 policy_"): encouraged but did not require code/data disclosure

::: {.incremental}

- Since July 2019 ("_Since 2019 policy_"): new Data and Code Disclosure policy

  - _”Authors of accepted papers ... must provide ... the data, programs, and other details ... sufficient to permit replication.”_

  - In March 2020: 
    - Establishment of the position of Code and Data Editor (CDE), plus a team of three Code & Data Associate Editors (CDAEs).

:::

::: {.notes}
  - Many exceptions:
    - Data with privacy issues (e.g., IRB does not allow sharing).
    - Data under NDA (e.g., field experiments with companies).
    - Proprietary (subscription) data (e.g., Comstat, CSRP, WRDS, etc.).

:::


## Current Code/Data Review at ManSci

::: {.incremental}

1. Authors submit their paper to the journal and complete a ”code & data disclosure form.”

2. After acceptance, authors submit the final manuscript and the _replication package_.

3. _Code & Data Editor team_ checks all accepted papers for the completeness of the replication package and verify if the code runs, when possible.<br>
$\rightarrow$ Not a full check for reproducibility, but mostly for completeness

4. Replication package approval $\rightarrow$ production

5. Replication package is published on the INFORMS server next to the article.

:::

::: {.notes}
2. The Decision Editor (DE) makes decisions about exceptions to the policy at
the acceptance stage.
:::


## ManSciReP: Management Science Reproducibility Project

- The Code & Data Editor team at ManSci reviews replication packages forcompleteness but does not test reproducibility.

ManSciReP: Large-scale project to assess the reproducibility of articles published in Management Science before and after 2019 policy change.


## Initial Sampling

__Articles and Replication Packages__

- All articles in Management Science that fell under the 2019 policy and were reviewed between April 2020 and January 2023.
  - 447 articles accepted/published, all of them with replication packages.

. . .

- As a comparison set, all articles that were accepted between January 2018 and April 2020 (before the new policy) would have fallen under the disclosure policy (i.e., include code or data).
  - 334 articles, of which 42 voluntarily provided a replication package.
  
. . .

::: {style="text-align:center;"}
$\rightarrow$ 781 articles in total
:::

. . .

::: {style="text-align:center;"}
$\rightarrow$ 489 articles with a replication package
:::

## Initial Sampling

__Reviewers__

- Volunteers, recruited after sending emails to ManSci reviewers/mailing lists.

. . .

::: {style="text-align:center;"}
$\rightarrow$ 935 researchers registered
:::

. . .

- Information collected on the typical department, software skills, and data access.
    

## Article-Reviewer Matching

Matching based on software skills and field of research

- Round 1 (Early Feb 2023)

  - Hungarian method (Kuhn, 1955, Hornik, 2005) to find a reviewer (out of 927) for each of the 489 packages. Penalties for mismatches in department, software skills, and database access, with a random resolution of ties
  
  - Matches were screened for potential conflicts of interest.
  
  - Email to matched reviewers with assignment details.

## Article-Reviewer Matching

Matching based on software skills and field of research

- Round 2 (Mid-Feb 2023)

  - Papers: previously unmatched papers (which received priority) and 2nd set of all papers.

  - Reviewers: All reviewers with no assignment yet, plus or reviewers available for another report.
  
  - Hungarian method with “regular” penalties for department and software mismatches and prohibitive penalties for assignments of the same paper or previous assignments.
  
  - Screened for conflicts of interests.
  
  - End of February: Email reminders to reviewers who did not confirm assignment yet.

## Article-Reviewer Matching

Matching based on software skills and field of research

- Round 3 (From March 2023):

  - Mostly manual matching, continuously in several waves.
  - Pool of papers: rejections of assignments, leftover
  - Pool of reviewers: unmatched reviewers, or reviewers available for another report.


## Reproducibility Assessment

- Reviewers were asked to:

  - make an “honest attempt” to reproduce the paper’s main results (figures, tables, other),
  
  - not to contact the original authors of the papers,
  
  - provide their report within 5 weeks.

. . .

- Structured report survey:

  - overall assessment
  
  - info about the content of the replication file (readme , data, code, etc.) and their quality

  - individual reproducibility assessment of all results tables and figures
  
  - time spent, own expertise in research field and analysis methods.

. . .

- Reviewers were also asked to provide log files and screenshots of their analysis.


::: {.notes}
- Throughout the process, reviewers were advised by email in case of questions.

- After submission, reports were screened for completeness and consistency.

:::


## Reviewers and Reports

:::: {.columns}

::: {.column width="50%"}

Reviewers:

- 675 reviewers submitted usable report(s) <br>
  (597 reviewers 1 report; 76 reviewers 2 reports; 2 reviewers 3 reports)
  
- 58 reports had a second reviewer assisting.

- In total, over 6,500 hours spent.

:::

::: {.column width="50%"}

Articles:

- 753 reports for 459 papers

<table>
<thead>
<tr>
  <th>Category</th>
  <th>Before 2019 policy</th>
  <th>After 2019 policy</th>
  <th>Total</th>
</tr>
</thead>
<tbody>
<tr>
  <td>Initial sample of articles</td>
  <td>334</td>
  <td>447</td>
  <td>781</td>
</tr>
<tr>
  <td>Replication package available</td>
  <td>42</td>
  <td>447</td>
  <td>489</td>
</tr>
<tr style="font-weight:bold;">
  <td>Package avail. and report</td>
  <td>40</td>
  <td>419</td>
  <td>459</td>
</tr>
<tr>
  <td>1 report</td>
  <td>16</td>
  <td>149</td>
  <td>165</td>
</tr>
<tr>
  <td>2 reports</td>
  <td>24</td>
  <td>270</td>
  <td>294</td>
</tr>
</tbody>
</table>


:::

::::


## Reviewers

<table style="text-align: center; border-collapse: collapse;">
    <tr>
        <th>N=675</th>
        <th>Share</th>
        <th>Enlisted</th>
        <th>Avg. Hours</th>
        <th>Avg. Expertise Method (0-100)</th>
        <th>Avg. Expertise Topic (0-100)</th>
    </tr>
    <tr>
        <td>Professor</td>
        <td>14%</td>
        <td>21%</td>
        <td>13.1</td>
        <td>84.3</td>
        <td>60.8</td>
    </tr>
    <tr>
        <td>Assoc. Prof.</td>
        <td>26%</td>
        <td>11%</td>
        <td>8.3</td>
        <td>83.2</td>
        <td>61.5</td>
    </tr>
    <tr>
        <td>Assist. Prof.</td>
        <td>40%</td>
        <td>6%</td>
        <td>8.4</td>
        <td>84.1</td>
        <td>58.7</td>
    </tr>
    <tr>
        <td>PhD student</td>
        <td>16%</td>
        <td>1%</td>
        <td>9.0</td>
        <td>83.8</td>
        <td>59.2</td>
    </tr>
    <tr>
        <td>Other</td>
        <td>4%</td>
        <td>3%</td>
        <td>6.1</td>
        <td>82.8</td>
        <td>52.7</td>
    </tr>
</table>


## Reviewed articles

<table style="font-size: 16pt; text-align: center; width: 100%; border-collapse: collapse;">
    <tr>
        <th><em>Management Science</em> Department</th>
        <th>Abbr.</th>
        <th style="text-align: center;">Share of Articles (N=489)</th>
        <th>Share of Reviewers (N=675)</th>
    </tr>
    <tr>
        <td>Finance</td>
        <td>FIN</td>
        <td>27.4%</td>
        <td>24.3%</td>
    </tr>
    <tr>
        <td>Behavioral Economics and Decision Analysis</td>
        <td>BDE</td>
        <td>18.4%</td>
        <td>30.1%</td>
    </tr>
    <tr>
        <td>Accounting</td>
        <td>ACC</td>
        <td>12.5%</td>
        <td>8.2%</td>
    </tr>
    <tr>
        <td>Operations Management</td>
        <td>OPM</td>
        <td>9.2%</td>
        <td>7.1%</td>
    </tr>
    <tr>
        <td>Marketing</td>
        <td>MKG</td>
        <td>5.7%</td>
        <td>6.5%</td>
    </tr>
    <tr>
        <td>Revenue Management and Market Analytics</td>
        <td>RMA</td>
        <td>4.7%</td>
        <td>0.7%</td>
    </tr>
    <tr>
        <td>Information Systems</td>
        <td>INS</td>
        <td>4.3%</td>
        <td>4.0%</td>
    </tr>
    <tr>
        <td>Business Strategy</td>
        <td>BST</td>
        <td>3.3%</td>
        <td>4.6%</td>
    </tr>
    <tr>
        <td>Healthcare Management</td>
        <td>HCM</td>
        <td>3.3%</td>
        <td>1.9%</td>
    </tr>
    <tr>
        <td>Big Data Analytics/Data Science</td>
        <td>BDA</td>
        <td>3.1%</td>
        <td>3.4%</td>
    </tr>
    <tr>
        <td>Organizations</td>
        <td>ORG</td>
        <td>3.1%</td>
        <td>3.6%</td>
    </tr>
    <tr>
        <td>Entrepreneurship and Innovation</td>
        <td>ENI</td>
        <td>2.3%</td>
        <td>4.0%</td>
    </tr>
    <tr>
        <td>Optimization</td>
        <td>OPT</td>
        <td>1.4%</td>
        <td>1.2%</td>
    </tr>
    <tr>
        <td>Stochastic Models and Simulations</td>
        <td>SMS</td>
        <td>1.4%</td>
        <td>0.4%</td>
    </tr>
</table>


# Results {.center background-color="#003366" style="text-align:center;"}


## Reproducibility assessment

__Assessment scale__

:::: {.columns}

::: {.column .incremental}

- Reproduced
  - Fully reproduced
    - exact same results for the whole article

  - Largely reproduced, with minor issues
    - small differences, but the article’s conclusions and learnings hold
    
:::

::: {.column}

:::

::::

## Reproducibility assessment {visibility="uncounted"}

__Assessment scale__

:::: {.columns}

::: {.column}

- Reproduced
  - Fully reproduced
    - exact same results for the whole article

  - Largely reproduced, with minor issues
    - small differences, but the article’s conclusions and learnings hold

:::

::: {.column .incremental}

- Not reproduced:

  - Largely not reproduced
    - major differences, conclusions of the original article not supported

  - Not reproduced  
    - conclusions of the original article not supported, no results reproduced
  <br><br>

  - Not verifiable (data n/a, requirements n/a)

  - Not verifiable (no package)


:::

::::


## Overall reproducibility

![](figures/reprod_overall.svg){fig-align="center"}


## Overall reproducibility

Linear probability model: **Largely or fully reproduced** (binary)

![](figures/table4_reg.png){fig-align="center"}

## Overall reproducibility {visibility="uncounted"}

Linear probability model: **Largely or fully reproduced** (binary)

![](figures/table4_reghl.png){fig-align="center"}




## Overall reproducibility by journal department

![](figures/reprod_departments.svg){fig-align="center"}

## Overall reproducibility by journal department {visibility="uncounted"}

![](figures/reprod_departments_highlight.svg){fig-align="center"}

## Overall reproducibility by study type / method

![](figures/reprod_methods.svg){fig-align="center"}


## Variation in reproducibility

:::: {.columns}

::: {.column width="38%"}
Linear probability model: <br> **Largely or fully reproduced** (binary)
:::

::: {.column width="62%"}
![](figures/table5_reg.png){fig-align="center"}
:::

::::


## Variation in reproducibility {visibility="uncounted"}

:::: {.columns}

::: {.column width="38%"}
Linear probability model: <br> **Largely or fully reproduced** (binary)
:::

::: {.column width="62%"}
![](figures/table5_reg1.png){fig-align="center"}
:::

::::


## Variation in reproducibility {visibility="uncounted"}

:::: {.columns}

::: {.column width="38%"}
Linear probability model: <br> **Largely or fully reproduced** (binary)
:::

::: {.column width="62%"}
![](figures/table5_reg2.png){fig-align="center"}
:::

::::


## Variation in reproducibility {visibility="uncounted"}

:::: {.columns}

::: {.column width="38%"}
Linear probability model: <br> **Largely or fully reproduced** (binary)
:::

::: {.column width="62%"}
![](figures/table5_reg3.png){fig-align="center"}
:::

::::


## Variation in reproducibility {visibility="uncounted"}

:::: {.columns}

::: {.column width="38%"}
Linear probability model: <br> **Largely or fully reproduced** (binary)
:::

::: {.column width="62%"}
![](figures/table5_reg3hl.png){fig-align="center"}
:::

::::


## Reasons for non-reproducibility

. . .

![](figures/nonreprod_reasons.svg){fig-align="center"}

## Reviewer consistency

<table>
    <tr>
        <td></td>
        <td>Fully</td>
        <td>Largely, minor</td>
        <td>Largely not r.</td>
        <td>Not r. but log</td>
        <td>Not r.</td>
    </tr>
    <tr>
        <td>Fully reproduced</td>
        <td>31</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Largely reproduced, minor issues</td>
        <td>64</td>
        <td>64</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Largely not reproduced, major issues</td>
        <td>6</td>
        <td>22</td>
        <td>7</td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td>Not rep. but consistent w/log</td>
        <td>1</td>
        <td>8</td>
        <td>3</td>
        <td>37</td>
        <td></td>
    </tr>
    <tr>
        <td>Not reproduced.</td>
        <td>2</td>
        <td>6</td>
        <td>14</td>
        <td>13</td>
        <td>18</td>
    </tr>
</table>

- Overall, no significant differences in self-assessed expertise in methods and topics between weakly more positive and weakly more negative reviewers (but in the “right” direction).


# Exploratory results

- Author and article characteristics

- Reviewer characteristics

- Reviewer expectations

- Data and code review


## Determinants of reproducibility

#### Data availability


:::: {.columns}

::: {.column width="68%"}
:::

:::  {.column width="32%"}

- No data:	14.87% <br> reproduced (n = 195)

- Partial data:	44.55% <br> reproduced (n = 110)

- All data:	92.11% <br> reproduced (n = 355)

:::

::::

## Determinants of reproducibility {visibility="uncounted"}

#### Data availability


:::: {.columns}

::: {.column width="68%"}

![](figures/reprod_by_data.svg){fig-align="center"}

:::

:::  {.column width="32%"}

- No data:	14.87% <br>reproduced (n = 195)

- Partial data:	44.55% <br>reproduced (n = 110)

- All data:	92.11% <br>reproduced (n = 355)

:::

::::


## Determinants of reproducibility

__Author affiliation Top 30__

- Top author affiliation: 		67.40% reproducible

- Non-top author affiliation: 	65.52% reproducible
<br>(t-test: p = 0.670, Fisher exact test: p = 0.693)

. . .

__Number of different software ($\rightarrow$ complexity)__

- One software used:		70.43% reproducible (n = 301)
- More than one software used:	60.17% reproducible (n = 118)
<br>(t-test: p = 0.044, Fisher exact test: p = 0.049)


::: aside
Top 30 school according to UT Dallas list
:::


## Determinants of reproducibility

__Reviewer affiliation Top 30__

- only 14% of reviewers from Top 30 schools, but 50% of papers (at least one author)
- Reviewer affiliation does not predict reproducibility (p = 0.811)

. . .

__Reviewer seniority__

| Position                     | Mean (%)  |
|------------------------------|-----------|
| PhD student (N=119)          | 63.9      |
| Assistant Prof (N=271)     | 63.5      |
| Associate Prof (N=173)       | 64.2      |
| Professor (N=95)             | 57.9      |
| Other (N=31)                 | 51.6      |





## Determinants of reproducibility

__Time spent on reproduction__

Mean time spent (only if largely or fully reproduced): 

<table border="1">
    <tr>
        <th>ACC</th>
        <th>BDA</th>
        <th>BDE</th>
        <th>BST</th>
        <th>ENI</th>
        <th>FIN</th>
        <th>HCM</th>
        <th>INS</th>
        <th>MKG</th>
        <th>OPM</th>
        <th>OPT</th>
        <th>ORG</th>
        <th>RMA</th>
        <th>SMS</th>
        <th><b>Total</b></th>
    </tr>
    <tr>
        <td>12.2</td>
        <td>13.8</td>
        <td>6.5</td>
        <td>11.7</td>
        <td>5.2</td>
        <td>12.3</td>
        <td>9.1</td>
        <td>5.9</td>
        <td>3.7</td>
        <td>6.6</td>
        <td>13.2</td>
        <td>5.9</td>
        <td>8.0</td>
        <td>3.7</td>
        <td><b>9.0</b></td>
    </tr>
</table>

<br>

. . .

:::: {.columns}

::: {.column width="40%"}

<table border="1">
    <tr>
        <th>Study type / method</th>
        <th>Mean</th>
    </tr>
    <tr>
        <td>Lab/online exper.</td>
        <td>5.7</td>
    </tr>
    <tr>
        <td>Field experiment</td>
        <td>5.5</td>
    </tr>
    <tr>
        <td>Survey</td>
        <td>3.9</td>
    </tr>
    <tr>
        <td>Empirical data</td>
        <td>11.0</td>
    </tr>
    <tr>
        <td>Theory/Simulation</td>
        <td>11.2</td>
    </tr>
</table>

:::

::: {.column  width="60%"}
:::

::::


## Determinants of reproducibility {visibility="uncounted}

__Time spent on reproduction__

Mean time spent (only if largely or fully reproduced): 

<table border="1">
    <tr>
        <th>ACC</th>
        <th>BDA</th>
        <th>BDE</th>
        <th>BST</th>
        <th>ENI</th>
        <th>FIN</th>
        <th>HCM</th>
        <th>INS</th>
        <th>MKG</th>
        <th>OPM</th>
        <th>OPT</th>
        <th>ORG</th>
        <th>RMA</th>
        <th>SMS</th>
        <th><b>Total</b></th>
    </tr>
    <tr>
        <td>12.2</td>
        <td>13.8</td>
        <td>6.5</td>
        <td>11.7</td>
        <td>5.2</td>
        <td>12.3</td>
        <td>9.1</td>
        <td>5.9</td>
        <td>3.7</td>
        <td>6.6</td>
        <td>13.2</td>
        <td>5.9</td>
        <td>8.0</td>
        <td>3.7</td>
        <td><b>9.0</b></td>
    </tr>
</table>

<br>

:::: {.columns}

::: {.column width="40%"}

<table border="1">
    <tr>
        <th>Study type / method</th>
        <th>Mean</th>
    </tr>
    <tr>
        <td>Lab/online exper.</td>
        <td>5.7</td>
    </tr>
    <tr>
        <td>Field experiment</td>
        <td>5.5</td>
    </tr>
    <tr>
        <td>Survey</td>
        <td>3.9</td>
    </tr>
    <tr>
        <td>Empirical data</td>
        <td>11.0</td>
    </tr>
    <tr>
        <td>Theory/Simulation</td>
        <td>11.2</td>
    </tr>
</table>

:::

::: {.column  width="60%"}

But: time spent has no sig. effect on overall reproducibility (OLS, p = 0.990)

:::

::::


## Estimating reproducibility

What proportion of Management Science articles _under the current Data & Code disclosure policy_ (...) can be fully reproduced with the available replication materials? 

. . .

:::: {.columns}

::: {.column width="70%"}
![](figures/pred_reprod.svg){fig-align="center"}
:::

::::


## Estimating reproducibility {visibility="uncounted"}

What proportion of Management Science articles _under the current Data & Code disclosure policy_ (...) can be fully reproduced with the available replication materials? 

:::: {.columns}

::: {.column width="70%"}
![](figures/pred_reprod.svg){fig-align="center"}
:::

::: {.column width="30%"}
$\rightarrow$ estimated reproducibility does not predict actual reproducibility <br> (reviewer level)
:::

::::


## Estimating reproducibility {auto-animate=true}

What proportion of Management Science articles _under the current Data & Code disclosure policy_ (...) can be fully reproduced with the available replication materials? 

. . .

- Mean estimated reproducibility since 2019 policy:	__72.61%__ (StdDev: 19.40)


## Estimating reproducibility {auto-animate=true visibility="uncounted"}


What proportion of Management Science articles _under the previous policy_ (...) can be fully reproduced with the available replication materials? 

- Mean estimated reproducibility before 2019 policy:	__48.20%__ (StdDev: 22.52)
<br><br>

What proportion of Management Science articles _under the current Data & Code disclosure policy_ (...) can be fully reproduced with the available replication materials? 

- Mean estimated reproducibility since 2019 policy:	__72.61%__ (StdDev: 19.40)




## Estimating reproducibility

#### Estimated reproducibility before/since 2019 policy 

![](figures/pred_reprod_by_position.svg){fig-align="center"}

## Estimating reproducibility

#### Estimated reproducibility before/since 2019 policy 

![](figures/pred_reprod_by_department.svg){fig-align="center"}


## Estimating reproducibility

#### Prediction errors by department (since 2019 policy)

![](figures/pred_error_by_department.svg){fig-align="center"}



## Reproducibility and replicability

- Does reproducibility imply replicability?

. . .

- $f_R(data, code) = \beta$ $\Longrightarrow$ $f_R(data', code'(method)) \approx \beta$ ? 

. . .

![](figures/pred_replic2.svg){fig-align="center"}


## Reproducibility and replicability {visibility="uncounted"}

- Does reproducibility imply replicability?
- $f_R(data, code) = \beta$ $\Longrightarrow$ $f_R(data', code'(method)) \approx \beta$ ? 

![](figures/pred_replic1.svg){fig-align="center"}



## Conclusion {.reverse .center background-color="#335c85" transition="slide-in none-out" auto-animate=true}

## Conclusion {.reverse .center background-color="#335c85" transition="slide-in none-out" auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="70%"}

- Estimate reproducibility in Management Science, overall and across different fields:
  - 68% of articles since 2019 policy can be fully or largely reproduced.
  - Reproducibility rates differ between 40% and 100%: e.g., BDE 88% vs. FIN 63%

:::

::::

## Conclusion {.reverse .center background-color="#335c85" transition="slide-in none-out" auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="70%"}

- Estimate reproducibility in Management Science, overall and across different fields:
  - 68% of articles since 2019 policy can be fully or largely reproduced.
  - Reproducibility rates differ between 40% and 100%: e.g., BDE 88% vs. FIN 63%

- Compare reproducibility before/since policy:
  - Only 7% of articles before 2019 policy can be reproduced.

:::

::::
  

## Conclusion {.reverse .center background-color="#335c85" transition="slide-in none-out" auto-animate=true visibility="uncounted"}

:::: {.columns}

::: {.column width="70%"}

- Estimate reproducibility in Management Science, overall and across different fields:
  - 68% of articles since 2019 policy can be fully or largely reproduced.
  - Reproducibility rates differ between 40% and 100%: e.g., BDE 88% vs. FIN 63%

- Compare reproducibility before/since policy:
  - Only 7% of articles before 2019 policy can be reproduced.
  
- Identify main reason for non-reproducibility:
  - no access to the data set (90%, proprietary / NDA / privacy / subscription).

:::

::::

::: {.notes}
  - If ignoring papers with limited data access, then 43% of papers can be fully reproduced, and 53\% can be largely reproduced (with minor issues).
:::

::: {.notes}

- For articles submitted since June 2019 (after a policy change making replication packages mandatory), 95% could be fully or largely computationally reproduced when data access and software requirements were met.

- Before the introduction of the 2019 disclosure policy, only 12% of articles voluntarily provided replication materials, of which 55% could be (largely) reproduced. So the policy change seems to have been effective in boosting reproducibility.

- Substantial heterogeneity in reproducibility rates across journal departments is mainly due to variations in methods and dataset accessibility specific to the research fields.

- Reproduction does not guarantee replicability. But it allows other researchers to scrutinize robustness, conduct their own (meta-)analysis, reuse code and data, and explore limitations of original results. This promotes scientific discourse and decreases incentives for fraud.

:::


## Future directions {auto-animate=true background-color="#003366"}

- experimental methods $\longleftrightarrow$ financial economics
- credibility and replicability in (social science) research


## Future directions {auto-animate=true background-color="#003366"}

- credibility and replicability in (social science) research
- experimental methods $\longleftrightarrow$ financial economics

. . .

- innovative methods and research designs
  - e.g., crowd-science approaches and meta analyses

. . .

- address external validity concerns
  - e.g., by conducting artefactual and framed field experiments <br>(experiments with financial professionals)

. . .

- novel questions originating from recent developments in the field 
  - e.g, cryptocurrency markets; how the COVID-19 crash affected decision making; deception in financial advice; attention and motivated information acquisition; competitiveness and speculation; ... 

  

## Conclusion {background-color="#003366"}


:::: {.columns}

::: {.column width="70%"}

- __Estimate reproducibility in Management Science, overall and across different fields:__
  - 68% of articles since 2019 policy can be fully or largely reproduced.
  - Reproducibility rates differ between 40% and 100%: e.g., BDE 88% vs. FIN 63%

- __Compare reproducibility before/since policy:__
  - Only 7% of articles before 2019 policy can be reproduced.
  
- __Identify main reason for non-reproducibility:__
  - no access to the data set (90%, proprietary / NDA / privacy / subscription).

:::

::: {.column width="30%"}

## Thanks!

christoph.huber@wu.ac.at
chr-huber.com

:::

::::

# Appendix {visibility="uncounted"}

## Reproducibility of tables {visibility="uncounted"}

![](figures/reprod_tables.svg){fig-align="center"}

## Reproducibility of figures {visibility="uncounted"}

![](figures/reprod_figures.svg){fig-align="center"}


## Reproducibility of other results {visibility="uncounted"}

![](figures/reprod_other_results.svg){fig-align="center"}

## Software {visibility="uncounted"}

![](figures/tableB1_software.png){fig-align="center"}

## Reasons for non-reproducibility {visibility="uncounted"}

![](figures/tableB2_reasons.png){fig-align="center"}

## Article type / method by journal department {visibility="uncounted"}

![](figures/tableB3_departmentmethod.png){fig-align="center"}

## Robustness {visibility="uncounted"}
 
![](figures/tableC1_robustness.png){fig-align="center"}

## Robustness {visibility="uncounted"}

![](figures/tableC2_regreportlevel.png){fig-align="center"}


## Robustness {visibility="uncounted"}

![](figures/tableC3_consistency.png){fig-align="center"}



